{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76f6f14f-ada4-4e34-8808-6ff1534445b4",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "Lucas Bezerra, ID: 171412,\n",
    "lucas.camaradantasbezerra@kaust.edu.sa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7c33b8-1ee2-40a9-a912-085694a83a70",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Problem 1: Transformer Questions\n",
    "\n",
    "<strong>Q1 (10 points): In the above self-attention operation, why we need to incorporate the scale factor âˆšdk into calculation?</strong>\n",
    "\n",
    "The scale factor is used for normalizing the dot product $Q\\,K^T$, making sure it has the same variance as the individual components of $Q$ and $K$.\n",
    "\n",
    "Assume these components are random variables $q_i, k_i\\;,\\forall i\\in\\{1,2,\\dots,d_k\\}$ with mean 0 and variance $\\sigma^2$. Then, it stems from the Central Limit Theorem that the dot product between $\\mathbf{q}$ and $\\mathbf{k}$ is normally-distributed:\n",
    "\n",
    "$$ \\mathbf{q}\\cdot \\mathbf{k} = \\sum\\limits_{i=1}^{d_k} q_i\\cdot k_i \\sim \\mathcal{N}\\left( 0, d_k\\cdot \\sigma^2 \\right) $$\n",
    "\n",
    "$$ \\frac{1}{\\sqrt{d_k}} \\mathbf{q}\\cdot \\mathbf{k} \\sim \\mathcal{N}\\left( 0,\\sigma^2 \\right) $$\n",
    "\n",
    "This ensures the network will have similar ranges of parameters no matter what dimensionality is chosen, and avoids vanishing gradient problems (keeping the variance of $\\mathbf{q}\\cdot \\mathbf{k}$ low ensures that $\\text{softmax}(\\mathbf{q}\\cdot \\mathbf{k})$ is also close to the origin, and thus in a high-gradient region)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289e0a64-5051-4e8a-9d5e-021cfc19bd7a",
   "metadata": {},
   "source": [
    "<strong>Q2 (10 points): When we train the Transformer on the word sequences, usually we need to add additional positional embedding for each word, why is this necessary?</strong>\n",
    "\n",
    "Without position encoding, the attention is an ordering-agnostic function, meaning that the ordering of words in a sentence does not change the outcome. However, in any language the order of words matters. That is why a positional embedding, that encodes the position in a way that enables the model to decode it, is added to each word embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73ea51a-9f42-4be0-a596-5ee919243a5f",
   "metadata": {},
   "source": [
    "<strong>Q3 (10 points): In the Transformer framework, there are two types of attention modules, which are self-attention and encoder-decoder attention. What is the difference between these two modules in terms of functionality and technical implementation?</strong>\n",
    "\n",
    "Both modules are very similar with respect to their operations on the data. However, while self-attention computes $Q,K,V$ (queries, keys, and values) from itself, the encoder-decoder attention block takes $K$ and $V$ (keys and values) from the last encoder block, and computes only $Q$ (queries) from itself. \n",
    "\n",
    "This means that, while self-attention relies on the keys and values of the input tokens and itself, the encoder-decoder attention relies on the keys and values that encode all tokens that were encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6362d6b-bb27-4741-bbf3-4549ec5d1bfa",
   "metadata": {},
   "source": [
    "<strong>Q4 (10 points): There are also other types of attention calculations such as the additive attention [4]. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. In the Transformer model, why the authors choose to use scaled-dot product attention instead of additive attention and what is the main advantages?</strong>\n",
    "\n",
    "In [2] the authors argue that, although both attention mechanisms are similar in theoretical complexity, the multiplicative attention is computationally more efficient, since it consists simple matrix multiplications, an operation that runs very fast in specialized hardware (e.g. GPUs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e731b9e-e496-4ee7-beb9-5e0dba258c20",
   "metadata": {},
   "source": [
    "<strong>Q5 (10 points): BERT and GPT model pretrain their model on a largescale dataset in a self-supervising way. Please describe their pretraining tasks and discuss why it is useful.</strong>\n",
    "\n",
    "In BERT, for every sentence around 15% of the words are masked and the model tries to predict which one that is, given all other words in the sentence. They call it Masked Language Model (MLM) pre-training objective. Since BERT's use cases provide the full sentence, e.g. for classification purposes, the MLM is a good method for making it learn how each word/token is related to the others. Additionally, BERT also employs Next Sentence Prediction (NSP), with the goal of training it to identify relationships between sentences, which is very important in applications such as question answering and natural language inference.\n",
    "\n",
    "In GPT, however, the goal is to generate words given the previous ones only. Since MLM allows for looking at almost all words in a sentence, independent of position, it cannot be used in GPT. It uses instead Next Word Prediction, where for each word, all words up to the current one are fed to the model and all upcoming ones are masked; the model has then to predict the next word. Since the upcoming words are masked, it has to make its prediction based on the previous ones only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b49473-e200-4cac-b27d-50f0331c8dca",
   "metadata": {},
   "source": [
    "<strong>Q6 (10 points) : In the BERT model design, there are two special tokens [CLS] and [SEP], what is the purpose of designing these two special tokens and how they are used during the training and evaluation.</strong>\n",
    "\n",
    "The [CLS] token is used at the beginning of sentences when the goal is to classify them. This token has a high attention to all other words, and thus the output at the first position is going to be an aggregate representation of all words, i.e. a representation that can be used for classfication purposes.\n",
    "\n",
    "The [SEP] token stands for sepparation and marks where one sentence ends and another starts. In use cases where multiple sentences are fed to the BERT model, this token is used to prevent that their contexts get mixed when aggregating words for the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b505b0b7-3345-42da-aa76-9450e6964549",
   "metadata": {},
   "source": [
    "## Problem 2: Coding Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1558601d-3ca5-410b-8389-69a3761c3869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,time,math,textwrap\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import dataset, transformer\n",
    "\n",
    "root = 'data/wikitext-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a3443b8-708a-416a-a437-46dbd1968c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .00035\n",
    "context = 150\n",
    "batch_size = 32\n",
    "log_interval = 50\n",
    "\n",
    "heads = 10\n",
    "depth = 16\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "795a5ec3-6795-40c1-a156-c5758225ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset.WikiText2(root, context, dataset.DatasetSplit.train)\n",
    "valid_data = dataset.WikiText2(root, context, dataset.DatasetSplit.valid)\n",
    "test_data = dataset.WikiText2(root, context, dataset.DatasetSplit.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9648ee1-44b8-4fd5-8c0e-bb6461db33ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0.\n",
    "        loader = torch.utils.data.DataLoader(dataset=data,batch_size=batch_size,shuffle=False)\n",
    "        for i, (x,y) in enumerate(loader):\n",
    "            x, y = x.permute(1,0).to(device), y.permute(1,0).to(device)\n",
    "            \n",
    "            yhat = model(x).view(-1, train_data.word_count())\n",
    "            loss += criterion(yhat, y.contiguous().view(-1))\n",
    "\n",
    "    print()\n",
    "    model.train()\n",
    "    return loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8f25b3d-a2bc-4fab-97e1-d08048c61f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized graph with 35211279 parameters\n"
     ]
    }
   ],
   "source": [
    "model = transformer.Transformer(context, train_data.word_count(), 400, 40, 900, heads, depth, tied_weights=True).to(device)\n",
    "count = sum([np.prod(parm.shape) for parm in model.parameters() if parm.requires_grad])\n",
    "print('Initialized graph with {} parameters'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea6e01f7-00a7-42a4-882c-2c4cab395bf9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating training, 436 iterations/epoch.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| checkpoint | epoch   0 | time:  3.77s | validation loss 10.42 | validation perplexity 33472.67\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
      "  0 (11.5%)\t259.63\t\t0.0001\t 8.79\t 6590.63\n",
      "  0 (22.9%)\t260.10\t\t0.0001\t 7.23\t 1376.32\n",
      "  0 (34.4%)\t260.87\t\t0.0001\t 6.67\t  787.96\n",
      "  0 (45.9%)\t261.89\t\t0.0001\t 6.44\t  627.30\n",
      "  0 (57.3%)\t262.01\t\t0.0001\t 6.31\t  550.15\n",
      "  0 (68.8%)\t261.84\t\t0.0001\t 6.22\t  501.68\n",
      "  0 (80.3%)\t262.02\t\t0.0001\t 6.12\t  453.23\n",
      "  0 (91.7%)\t262.07\t\t0.0001\t 6.06\t  429.47\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| checkpoint | epoch   1 | time:  3.38s | validation loss  5.84 | validation perplexity   344.88\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
      "  1 (11.5%)\t262.90\t\t0.00035\t 5.94\t  378.05\n",
      "  1 (22.9%)\t262.33\t\t0.00035\t 5.84\t  344.19\n",
      "  1 (34.4%)\t262.33\t\t0.00035\t 5.76\t  316.00\n",
      "  1 (45.9%)\t262.51\t\t0.00035\t 5.66\t  287.10\n",
      "  1 (57.3%)\t262.37\t\t0.00035\t 5.58\t  265.80\n",
      "  1 (68.8%)\t262.19\t\t0.00035\t 5.52\t  249.78\n",
      "  1 (80.3%)\t262.06\t\t0.00035\t 5.44\t  230.53\n",
      "  1 (91.7%)\t261.77\t\t0.00035\t 5.36\t  212.38\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| checkpoint | epoch   2 | time:  3.39s | validation loss  5.24 | validation perplexity   189.20\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
      "  2 (11.5%)\t262.45\t\t0.00035\t 5.06\t  158.08\n",
      "  2 (22.9%)\t262.27\t\t0.00035\t 5.07\t  159.07\n",
      "  2 (34.4%)\t262.44\t\t0.00035\t 5.03\t  152.26\n",
      "  2 (45.9%)\t262.58\t\t0.00035\t 4.99\t  147.60\n",
      "  2 (57.3%)\t262.29\t\t0.00035\t 4.96\t  141.94\n",
      "  2 (68.8%)\t262.55\t\t0.00035\t 4.92\t  137.29\n",
      "  2 (80.3%)\t262.53\t\t0.00035\t 4.90\t  134.83\n",
      "  2 (91.7%)\t262.88\t\t0.00035\t 4.84\t  126.25\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| checkpoint | epoch   3 | time:  3.39s | validation loss  4.97 | validation perplexity   143.68\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
      "  3 (11.5%)\t261.81\t\t0.00035\t 4.57\t   96.95\n",
      "  3 (22.9%)\t262.28\t\t0.00035\t 4.58\t   97.28\n",
      "  3 (34.4%)\t262.08\t\t0.00035\t 4.54\t   93.30\n",
      "  3 (45.9%)\t261.95\t\t0.00035\t 4.52\t   91.80\n",
      "  3 (57.3%)\t262.24\t\t0.00035\t 4.52\t   92.08\n",
      "  3 (68.8%)\t262.50\t\t0.00035\t 4.53\t   92.51\n",
      "  3 (80.3%)\t262.76\t\t0.00035\t 4.49\t   89.53\n",
      "  3 (91.7%)\t262.23\t\t0.00035\t 4.48\t   88.34\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| checkpoint | epoch   4 | time:  3.39s | validation loss  4.80 | validation perplexity   121.63\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
      "  4 (11.5%)\t284.39\t\t0.00035\t 4.17\t   64.63\n",
      "  4 (22.9%)\t284.06\t\t0.00035\t 4.15\t   63.64\n",
      "  4 (34.4%)\t284.40\t\t0.00035\t 4.18\t   65.05\n",
      "  4 (45.9%)\t284.69\t\t0.00035\t 4.16\t   64.05\n",
      "  4 (57.3%)\t284.39\t\t0.00035\t 4.18\t   65.11\n",
      "  4 (68.8%)\t285.31\t\t0.00035\t 4.19\t   66.09\n",
      "  4 (80.3%)\t284.94\t\t0.00035\t 4.20\t   66.46\n",
      "  4 (91.7%)\t285.15\t\t0.00035\t 4.17\t   64.53\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| checkpoint | epoch   5 | time:  3.39s | validation loss  4.74 | validation perplexity   114.48\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
      "  5 (11.5%)\t284.18\t\t0.00035\t 3.80\t   44.49\n",
      "  5 (22.9%)\t283.74\t\t0.00035\t 3.82\t   45.67\n",
      "  5 (34.4%)\t284.46\t\t0.00035\t 3.87\t   48.07\n",
      "  5 (45.9%)\t284.50\t\t0.00035\t 3.87\t   47.84\n",
      "  5 (57.3%)\t284.43\t\t0.00035\t 3.87\t   48.15\n",
      "  5 (68.8%)\t285.23\t\t0.00035\t 3.89\t   48.86\n",
      "  5 (80.3%)\t285.10\t\t0.00035\t 3.88\t   48.62\n",
      "  5 (91.7%)\t284.91\t\t0.00035\t 3.89\t   48.79\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| checkpoint | epoch   6 | time:  3.40s | validation loss  4.75 | validation perplexity   116.10\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
      "  6 (11.5%)\t285.69\t\t0.00035\t 3.49\t   32.78\n",
      "  6 (22.9%)\t285.52\t\t0.00035\t 3.50\t   33.18\n",
      "  6 (34.4%)\t286.10\t\t0.00035\t 3.53\t   34.24\n",
      "  6 (45.9%)\t285.62\t\t0.00035\t 3.56\t   35.18\n",
      "  6 (57.3%)\t285.55\t\t0.00035\t 3.58\t   35.99\n",
      "  6 (68.8%)\t285.80\t\t0.00035\t 3.60\t   36.46\n",
      "  6 (80.3%)\t285.69\t\t0.00035\t 3.63\t   37.55\n",
      "  6 (91.7%)\t285.92\t\t0.00035\t 3.63\t   37.76\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| checkpoint | epoch   7 | time:  3.39s | validation loss  4.78 | validation perplexity   119.22\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
      "  7 (11.5%)\t286.00\t\t0.00035\t 3.18\t   23.97\n",
      "  7 (22.9%)\t286.03\t\t0.00035\t 3.22\t   25.11\n",
      "  7 (34.4%)\t285.81\t\t0.00035\t 3.25\t   25.87\n",
      "  7 (45.9%)\t285.55\t\t0.00035\t 3.28\t   26.60\n",
      "  7 (57.3%)\t285.89\t\t0.00035\t 3.30\t   27.15\n",
      "  7 (68.8%)\t285.58\t\t0.00035\t 3.34\t   28.18\n",
      "  7 (80.3%)\t285.65\t\t0.00035\t 3.34\t   28.32\n",
      "  7 (91.7%)\t284.94\t\t0.00035\t 3.37\t   29.12\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| checkpoint | epoch   8 | time:  3.40s | validation loss  4.87 | validation perplexity   130.43\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
      "  8 (11.5%)\t285.87\t\t0.00035\t 2.90\t   18.22\n",
      "  8 (22.9%)\t285.56\t\t0.00035\t 2.92\t   18.59\n",
      "  8 (34.4%)\t286.02\t\t0.00035\t 2.97\t   19.54\n",
      "  8 (45.9%)\t286.21\t\t0.00035\t 3.01\t   20.23\n",
      "  8 (57.3%)\t286.58\t\t0.00035\t 3.04\t   20.81\n",
      "  8 (68.8%)\t286.07\t\t0.00035\t 3.07\t   21.51\n",
      "  8 (80.3%)\t285.89\t\t0.00035\t 3.08\t   21.86\n",
      "  8 (91.7%)\t286.03\t\t0.00035\t 3.09\t   22.07\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| checkpoint | epoch   9 | time:  3.39s | validation loss  5.04 | validation perplexity   154.76\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
      "  9 (11.5%)\t286.18\t\t0.00035\t 2.62\t   13.80\n",
      "  9 (22.9%)\t285.95\t\t0.00035\t 2.65\t   14.09\n",
      "  9 (34.4%)\t285.61\t\t0.00035\t 2.68\t   14.59\n",
      "  9 (45.9%)\t285.95\t\t0.00035\t 2.72\t   15.23\n",
      "  9 (57.3%)\t285.85\t\t0.00035\t 2.77\t   15.89\n",
      "  9 (68.8%)\t285.62\t\t0.00035\t 2.81\t   16.58\n",
      "  9 (80.3%)\t285.95\t\t0.00035\t 2.82\t   16.83\n",
      "  9 (91.7%)\t285.83\t\t0.00035\t 2.84\t   17.05\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "curr_lr = .0001\n",
    "clip = .25\n",
    "best_val_loss = None\n",
    "epochs = 10\n",
    "save = 'model.pt'\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data,batch_size=batch_size,shuffle=True)\n",
    "print('Initiating training, {} iterations/epoch.'.format(len(train_loader)))\n",
    "\n",
    "try:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=curr_lr)\n",
    "    for epoch in range(epochs):\n",
    "        t0 = time.time()\n",
    "        val_loss = evaluate(valid_data)\n",
    "        print('-' * 100)\n",
    "        print('| checkpoint | epoch {:3d} | time: {:5.2f}s | validation loss {:5.2f} | '\n",
    "                'validation perplexity {:8.2f}'.format(epoch, (time.time() - t0),\n",
    "                                                       val_loss, math.exp(val_loss)))\n",
    "        print('-' * 100)\n",
    "        print('epoch\\t\\tms/batch\\tlr\\tloss\\tperplexity')\n",
    "\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0.\n",
    "        t0 = time.time()\n",
    "        if epoch == 1: optimizer.param_groups[0]['lr'] = curr_lr = lr # finished warmup\n",
    "        for i, (x,y) in enumerate(train_loader):\n",
    "            if i % log_interval == 0 and i > 0:\n",
    "                cur_loss = total_loss / log_interval\n",
    "                elapsed = time.time() - t0\n",
    "                print('{:3d} ({:2.1f}%)\\t{:5.2f}\\t\\t{:1.3}\\t{:5.2f}\\t{:8.2f}'.format(\n",
    "                    epoch, 100*i/float(len(train_loader)),\n",
    "                    elapsed * 1000 / log_interval, curr_lr, cur_loss, math.exp(cur_loss)))\n",
    "                total_loss = 0\n",
    "                t0 = time.time()\n",
    "\n",
    "            x, y = x.permute(1,0).to(device), y.permute(1,0).to(device)\n",
    "            model.zero_grad()\n",
    "            \n",
    "            yhat = model(x).view(-1, train_data.word_count())\n",
    "            loss = criterion(yhat, y.contiguous().view(-1))\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('Graceful Exit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65f94cd8-db85-4691-a1b4-76d7e7db134e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring best checkpointed model...\n",
      "\n",
      "=========================================================================================\n",
      "| end of training | test loss  4.69 | test perplexity   109.35\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "print('Restoring best checkpointed model...')\n",
    "with open(save, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "\n",
    "test_loss = evaluate(test_data)\n",
    "print('=' * 89)\n",
    "print('| end of training | test loss {:5.2f} | test perplexity {:8.2f}'.format(test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "761e191d-35e8-442e-850d-48629e98b962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uncurated samples\n",
      "-----------------------------------------------------------------------------------------\n",
      "(0) accepted lessons , generating similarities to the building . He reissued the\n",
      "position in the Chronicles III , in unsuccessfully to retire minesweepers the\n",
      "rhymes . <eos> <eos> = = = Wells Wells , and developing this industry as\n",
      "Santiago , known by <unk> , spoke to the play \" Second World \" to be in which\n",
      "Glenn , was the first play in Rome . He later established a Red Emergency\n",
      "association with the Welsh musicians , for example the time , performing over\n",
      "the 22nd and 21st Infantry Division in the Austro @-@ century before the hearts\n",
      "he saw me in a sum from another volume by 1 US that ever had been nominated . He\n",
      "featured a \" <unk> \" to the true hero ( b @-@ MM \" ) , which was written by\n",
      "Daniel of the Soviets , but a disturbing part of the show\n",
      "(1) 's performances in Hereford Abbey , Long Applewhite 's performance was described\n",
      "as \" Things from Spain \" but said that it was with \" true \" ritual or spreading\n",
      "\" ; Robert <unk> described his books variously as and added panels . Montoneros\n",
      "published non @-@ religious innovations in the day and was published in the\n",
      "1850s , and published it concurrently with increasingly Palacios who writes : he\n",
      "was derided in a critic and biographer Louis ; he later returned to his\n",
      "translation into the Middle Ages . He then met two than a few years of Delaborde\n",
      ", and one of his royalty had shifted to the Congo . When BolÃ­var was the stela\n",
      "carving of the corn crake , Hornung 's Measure was published , starring in Mary\n",
      ", Ronald at the D.C. Wheeler Connie , Bishop of Cambridge , Dean , and <unk> .\n",
      "The\n",
      "(2) . <eos> <eos> <eos> = = Reviews and individual writers explain Innis 's book '\n",
      "in Innis : <eos> <eos> <eos> Nixon commanded Nesbitt about the indie scholar in\n",
      "August 1979 , in which he found his career because he could \" Tis a wealthy gap\n",
      "beyond an obligation to <unk> . \" The Bootleg Series , the American composer and\n",
      "American novels featuring Lucille Ball , and Dennis <unk> , published in Michael\n",
      "Innis 's requires romantic comedy comedy . Biographer Adam Gill of his disciple\n",
      "stood on the sequel , commenting that parliamentary views the recognize John\n",
      "John if he <unk> him ; he has very much response to his autobiography that the\n",
      "historian 's Office of the Guardian described the novel as \" one of the best @-@\n",
      "auction show \" . His biographer , Irish Arnie Roth , journalist , academics and\n",
      "directing economics , contacted\n",
      "(3) â€” typically thorns on hearing reason . \" sexual acts has speculated on reason ,\n",
      "\" Reyes <unk> , Minervois <unk> \" and <unk> \" . <unk> writes that Sanskrit\n",
      "character , <unk> Gillespie , <unk> , historian of narrative , notes that \"\n",
      "recur within the best way of being of the obituary for reports by the <unk> of\n",
      "that demonstrated goodbye suicide . <unk> correctly Coco managed to prove\n",
      "himself and realizes that he had a legal ambition . <unk> , Turkey has doubts\n",
      "about killing even marketing books similar to his mansion 's earlier title \"\n",
      "constitutional wife \" , which <unk> from <unk> the <unk> of the New England\n",
      "Wonder . <unk> identify elements of students , seemingly changing the\n",
      "publication of the city , the Medway of The Being Being Earnest 's greatest\n",
      "productions ... Rowson notes that all the introduction he was not inspired\n",
      "(4) Series . <eos> <unk> , Japanese of the <unk> Tag Team Championship was '\n",
      "Champion , Andy Andy <unk> in a whole history which was played in the Swiss\n",
      "<unk> title , New York Helms played his leading 33 debut during age Songbook .\n",
      "The second part titled Live Arcade music and a Bigger and Better Better prose\n",
      "collection tour of New York Times praised praised USA News , which \" [ much ]\n",
      "<unk> and Paradise tribute \" \" , expressing which would have also been quotation\n",
      "from The One Sweet Day episode of the show Star Now Fringe , Rolling Stone felt\n",
      "that while \" Ever Monster \" is important at thy name controls \" , the episode \"\n",
      "masterly performance that pointless \" suffered an <unk> in the show \" <unk> \" ,\n",
      "\" The actual labeled \" , which \" Keamy was the \" hyper entry\n"
     ]
    }
   ],
   "source": [
    "print('\\nUncurated samples')\n",
    "print('-' * 89)\n",
    "\n",
    "def sample():\n",
    "    words = []\n",
    "    model.eval()\n",
    "    history = torch.randint(train_data.word_count(), (1, 1), dtype=torch.long).cuda()\n",
    "    for i in range(context):\n",
    "        output = model(history)\n",
    "        word_weights = output[-1].squeeze().exp().cpu()\n",
    "        word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "        word_tensor = torch.Tensor([[word_idx]]).long().cuda()\n",
    "        history = torch.cat([history, word_tensor], 0)\n",
    "\n",
    "        words.append(train_data.idx2word[word_idx])\n",
    "\n",
    "    return '\\n'.join(textwrap.wrap(' '.join(words),80))\n",
    "\n",
    "for i in range(5):\n",
    "    print('({})'.format(i), sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66a4f538-8c9e-48ff-aa5b-031557cb53aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e66541",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
