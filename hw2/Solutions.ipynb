{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76f6f14f-ada4-4e34-8808-6ff1534445b4",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "Lucas Bezerra, ID: 171412,\n",
    "lucas.camaradantasbezerra@kaust.edu.sa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7c33b8-1ee2-40a9-a912-085694a83a70",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Problem 1: Transformer Questions\n",
    "\n",
    "<strong>Q1 (10 points): In the above self-attention operation, why we need to incorporate the scale factor âˆšdk into calculation?</strong>\n",
    "\n",
    "The scale factor is used for normalizing the dot product $Q\\,K^T$, making sure it has the same variance as the individual components of $Q$ and $K$.\n",
    "\n",
    "Assume these components are random variables $q_i, k_i\\;,\\forall i\\in\\{1,2,\\dots,d_k\\}$ with mean 0 and variance $\\sigma^2$. Then, it stems from the Central Limit Theorem that the dot product between $\\mathbf{q}$ and $\\mathbf{k}$ is normally-distributed:\n",
    "\n",
    "$$ \\mathbf{q}\\cdot \\mathbf{k} = \\sum\\limits_{i=1}^{d_k} q_i\\cdot k_i \\sim \\mathcal{N}\\left( 0, d_k\\cdot \\sigma^2 \\right) $$\n",
    "\n",
    "$$ \\frac{1}{\\sqrt{d_k}} \\mathbf{q}\\cdot \\mathbf{k} \\sim \\mathcal{N}\\left( 0,\\sigma^2 \\right) $$\n",
    "\n",
    "This ensures the network will have similar ranges of parameters no matter what dimensionality is chosen, and avoids vanishing gradient problems (keeping the variance of $\\mathbf{q}\\cdot \\mathbf{k}$ low ensures that $\\text{softmax}(\\mathbf{q}\\cdot \\mathbf{k})$ is also close to the origin, and thus in a high-gradient region)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289e0a64-5051-4e8a-9d5e-021cfc19bd7a",
   "metadata": {},
   "source": [
    "<strong>Q2 (10 points): When we train the Transformer on the word sequences, usually we need to add additional positional embedding for each word, why is this necessary?</strong>\n",
    "\n",
    "Without position encoding, the attention is an ordering-agnostic function, meaning that the ordering of words in a sentence does not change the outcome. However, in any language the order of words matters. That is why a positional embedding is added to each word embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73ea51a-9f42-4be0-a596-5ee919243a5f",
   "metadata": {},
   "source": [
    "<strong>Q3 (10 points): In the Transformer framework, there are two types of attention modules, which are self-attention and encoder-decoder attention. What is the difference between these two modules in terms of functionality and technical implementation?</strong>\n",
    "\n",
    "TO DO ---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6362d6b-bb27-4741-bbf3-4549ec5d1bfa",
   "metadata": {},
   "source": [
    "<strong>Q4 (10 points): There are also other types of attention calculations such as the additive attention [4]. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. In the Transformer model, why the authors choose to use scaled-dot product attention instead of additive attention and what is the main advantages?</strong>\n",
    "\n",
    "In [2] the authors argue that, although both attention mechanisms are similar in theoretical complexity, the multiplicative attention is computationally more efficient, since it consists simple matrix multiplications, an operation that runs very fast in specialized hardware (e.g. GPUs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e731b9e-e496-4ee7-beb9-5e0dba258c20",
   "metadata": {},
   "source": [
    "<strong>Q5 (10 points): BERT and GPT model pretrain their model on a largescale dataset in a self-supervising way. Please describe their pretraining tasks and discuss why it is useful.</strong>\n",
    "\n",
    "TO DO ---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b49473-e200-4cac-b27d-50f0331c8dca",
   "metadata": {},
   "source": [
    "<strong>Q6 (10 points) : In the BERT model design, there are two special tokens \\[CLS\\] and [SEP], what is the purpose of designing these two special tokens and how they are used during the training and evaluation.</strong>\n",
    "\n",
    "TO DO ---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b505b0b7-3345-42da-aa76-9450e6964549",
   "metadata": {},
   "source": [
    "## Problem 2: Coding Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1558601d-3ca5-410b-8389-69a3761c3869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,time,math,textwrap\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import dataset, transformer\n",
    "\n",
    "root = 'data/wikitext-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a3443b8-708a-416a-a437-46dbd1968c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .00035\n",
    "context = 150\n",
    "batch_size = 32\n",
    "log_interval = 50\n",
    "\n",
    "heads = 10\n",
    "depth = 16\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "795a5ec3-6795-40c1-a156-c5758225ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset.WikiText2(root, context, dataset.DatasetSplit.train)\n",
    "valid_data = dataset.WikiText2(root, context, dataset.DatasetSplit.valid)\n",
    "test_data = dataset.WikiText2(root, context, dataset.DatasetSplit.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9648ee1-44b8-4fd5-8c0e-bb6461db33ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0.\n",
    "        loader = torch.utils.data.DataLoader(dataset=data,batch_size=batch_size,shuffle=False)\n",
    "        for i, (x,y) in enumerate(loader):\n",
    "            x, y = x.permute(1,0).to(device), y.permute(1,0).to(device)\n",
    "            \n",
    "            yhat = model(x).view(-1, train_data.word_count())\n",
    "            loss += criterion(yhat, y.contiguous().view(-1))\n",
    "\n",
    "    print()\n",
    "    model.train()\n",
    "    return loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8f25b3d-a2bc-4fab-97e1-d08048c61f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized graph with 35211279 parameters\n"
     ]
    }
   ],
   "source": [
    "model = transformer.Transformer(context, train_data.word_count(), 400, 40, 900, heads, depth, tied_weights=True).to(device)\n",
    "count = sum([np.prod(parm.shape) for parm in model.parameters() if parm.requires_grad])\n",
    "print('Initialized graph with {} parameters'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea6e01f7-00a7-42a4-882c-2c4cab395bf9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating training, 436 iterations/epoch.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| checkpoint | epoch   0 | time:  3.06s | validation loss 10.41 | validation perplexity 33192.81\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
      "  0 (11.5%)\t203.95\t\t0.0001\t 8.72\t 6099.21\n",
      "  0 (22.9%)\t202.72\t\t0.0001\t 7.21\t 1346.28\n",
      "  0 (34.4%)\t204.46\t\t0.0001\t 6.65\t  772.18\n",
      "  0 (45.9%)\t205.17\t\t0.0001\t 6.43\t  617.50\n",
      "  0 (57.3%)\t205.05\t\t0.0001\t 6.30\t  544.11\n",
      "  0 (68.8%)\t206.30\t\t0.0001\t 6.21\t  495.88\n",
      "  0 (80.3%)\t206.39\t\t0.0001\t 6.10\t  445.62\n",
      "  0 (91.7%)\t208.24\t\t0.0001\t 6.04\t  419.24\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| checkpoint | epoch   1 | time:  2.32s | validation loss  5.80 | validation perplexity   330.50\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
      "  1 (11.5%)\t207.27\t\t0.00035\t 5.94\t  381.62\n",
      "  1 (22.9%)\t207.54\t\t0.00035\t 5.83\t  340.73\n",
      "  1 (34.4%)\t208.02\t\t0.00035\t 5.74\t  312.56\n",
      "  1 (45.9%)\t208.60\t\t0.00035\t 5.65\t  283.84\n",
      "  1 (57.3%)\t208.74\t\t0.00035\t 5.56\t  260.94\n",
      "  1 (68.8%)\t208.62\t\t0.00035\t 5.50\t  244.07\n",
      "  1 (80.3%)\t208.84\t\t0.00035\t 5.41\t  224.72\n",
      "  1 (91.7%)\t209.43\t\t0.00035\t 5.33\t  205.76\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| checkpoint | epoch   2 | time:  2.35s | validation loss  5.20 | validation perplexity   181.97\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
      "  2 (11.5%)\t210.21\t\t0.00035\t 5.02\t  151.91\n",
      "  2 (22.9%)\t210.56\t\t0.00035\t 5.01\t  149.92\n",
      "  2 (34.4%)\t210.33\t\t0.00035\t 4.96\t  141.95\n",
      "  2 (45.9%)\t210.72\t\t0.00035\t 4.92\t  136.86\n",
      "  2 (57.3%)\t210.68\t\t0.00035\t 4.87\t  130.45\n",
      "  2 (68.8%)\t210.61\t\t0.00035\t 4.84\t  125.90\n",
      "  2 (80.3%)\t210.51\t\t0.00035\t 4.81\t  122.48\n",
      "  2 (91.7%)\t210.85\t\t0.00035\t 4.73\t  113.82\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| checkpoint | epoch   3 | time:  2.36s | validation loss  4.83 | validation perplexity   124.88\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
      "  3 (11.5%)\t210.53\t\t0.00035\t 4.44\t   84.37\n",
      "  3 (22.9%)\t210.33\t\t0.00035\t 4.42\t   83.31\n",
      "  3 (34.4%)\t210.43\t\t0.00035\t 4.38\t   79.91\n",
      "  3 (45.9%)\t209.05\t\t0.00035\t 4.36\t   78.44\n",
      "  3 (57.3%)\t210.22\t\t0.00035\t 4.36\t   78.13\n",
      "  3 (68.8%)\t209.17\t\t0.00035\t 4.35\t   77.23\n",
      "  3 (80.3%)\t209.25\t\t0.00035\t 4.32\t   75.37\n",
      "  3 (91.7%)\t209.87\t\t0.00035\t 4.32\t   75.09\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| checkpoint | epoch   4 | time:  2.36s | validation loss  4.65 | validation perplexity   104.40\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
      "  4 (11.5%)\t210.83\t\t0.00035\t 3.98\t   53.77\n",
      "  4 (22.9%)\t210.99\t\t0.00035\t 3.97\t   52.90\n",
      "  4 (34.4%)\t210.74\t\t0.00035\t 3.98\t   53.56\n",
      "  4 (45.9%)\t210.79\t\t0.00035\t 3.96\t   52.54\n",
      "  4 (57.3%)\t210.53\t\t0.00035\t 3.98\t   53.29\n",
      "  4 (68.8%)\t210.21\t\t0.00035\t 3.98\t   53.68\n",
      "  4 (80.3%)\t209.68\t\t0.00035\t 3.99\t   53.85\n",
      "  4 (91.7%)\t210.61\t\t0.00035\t 3.96\t   52.24\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| checkpoint | epoch   5 | time:  2.36s | validation loss  4.55 | validation perplexity    94.24\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
      "  5 (11.5%)\t210.64\t\t0.00035\t 3.63\t   37.61\n",
      "  5 (22.9%)\t210.99\t\t0.00035\t 3.64\t   38.01\n",
      "  5 (34.4%)\t210.75\t\t0.00035\t 3.68\t   39.79\n",
      "  5 (45.9%)\t210.69\t\t0.00035\t 3.68\t   39.73\n",
      "  5 (57.3%)\t210.88\t\t0.00035\t 3.69\t   39.90\n",
      "  5 (68.8%)\t211.07\t\t0.00035\t 3.69\t   39.96\n",
      "  5 (80.3%)\t210.67\t\t0.00035\t 3.69\t   40.02\n",
      "  5 (91.7%)\t211.06\t\t0.00035\t 3.70\t   40.29\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| checkpoint | epoch   6 | time:  2.36s | validation loss  4.51 | validation perplexity    90.58\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
      "  6 (11.5%)\t210.67\t\t0.00035\t 3.38\t   29.42\n",
      "  6 (22.9%)\t210.94\t\t0.00035\t 3.39\t   29.61\n",
      "  6 (34.4%)\t210.93\t\t0.00035\t 3.42\t   30.50\n",
      "  6 (45.9%)\t210.84\t\t0.00035\t 3.43\t   30.87\n",
      "  6 (57.3%)\t210.95\t\t0.00035\t 3.45\t   31.61\n",
      "  6 (68.8%)\t211.02\t\t0.00035\t 3.45\t   31.46\n",
      "  6 (80.3%)\t210.90\t\t0.00035\t 3.48\t   32.38\n",
      "  6 (91.7%)\t210.78\t\t0.00035\t 3.48\t   32.44\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| checkpoint | epoch   7 | time:  2.36s | validation loss  4.50 | validation perplexity    90.45\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
      "  7 (11.5%)\t211.10\t\t0.00035\t 3.16\t   23.67\n",
      "  7 (22.9%)\t210.95\t\t0.00035\t 3.21\t   24.68\n",
      "  7 (34.4%)\t211.09\t\t0.00035\t 3.21\t   24.85\n",
      "  7 (45.9%)\t210.70\t\t0.00035\t 3.24\t   25.49\n",
      "  7 (57.3%)\t211.01\t\t0.00035\t 3.25\t   25.81\n",
      "  7 (68.8%)\t211.12\t\t0.00035\t 3.28\t   26.45\n",
      "  7 (80.3%)\t211.06\t\t0.00035\t 3.28\t   26.47\n",
      "  7 (91.7%)\t211.05\t\t0.00035\t 3.29\t   26.92\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| checkpoint | epoch   8 | time:  2.36s | validation loss  4.51 | validation perplexity    90.64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
      "  8 (11.5%)\t210.72\t\t0.00035\t 2.99\t   19.98\n",
      "  8 (22.9%)\t211.17\t\t0.00035\t 3.01\t   20.32\n",
      "  8 (34.4%)\t211.04\t\t0.00035\t 3.04\t   20.93\n",
      "  8 (45.9%)\t210.98\t\t0.00035\t 3.07\t   21.53\n",
      "  8 (57.3%)\t211.02\t\t0.00035\t 3.07\t   21.52\n",
      "  8 (68.8%)\t211.00\t\t0.00035\t 3.08\t   21.74\n",
      "  8 (80.3%)\t211.00\t\t0.00035\t 3.07\t   21.61\n",
      "  8 (91.7%)\t211.05\t\t0.00035\t 3.04\t   20.94\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| checkpoint | epoch   9 | time:  2.36s | validation loss  4.31 | validation perplexity    74.59\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch\t\tms/batch\tlr\tloss\tperplexity\n",
      "  9 (11.5%)\t210.82\t\t0.00035\t 2.64\t   13.98\n",
      "  9 (22.9%)\t210.95\t\t0.00035\t 2.58\t   13.13\n",
      "  9 (34.4%)\t210.86\t\t0.00035\t 2.46\t   11.70\n",
      "  9 (45.9%)\t210.95\t\t0.00035\t 2.25\t    9.45\n",
      "  9 (57.3%)\t211.07\t\t0.00035\t 1.84\t    6.31\n",
      "  9 (68.8%)\t211.01\t\t0.00035\t 1.31\t    3.71\n",
      "  9 (80.3%)\t210.74\t\t0.00035\t 0.89\t    2.42\n",
      "  9 (91.7%)\t210.95\t\t0.00035\t 0.63\t    1.87\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "curr_lr = .0001\n",
    "clip = .25\n",
    "best_val_loss = None\n",
    "epochs = 10\n",
    "save = 'model.pt'\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data,batch_size=batch_size,shuffle=True)\n",
    "print('Initiating training, {} iterations/epoch.'.format(len(train_loader)))\n",
    "\n",
    "try:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=curr_lr)\n",
    "    for epoch in range(epochs):\n",
    "        t0 = time.time()\n",
    "        val_loss = evaluate(valid_data)\n",
    "        print('-' * 100)\n",
    "        print('| checkpoint | epoch {:3d} | time: {:5.2f}s | validation loss {:5.2f} | '\n",
    "                'validation perplexity {:8.2f}'.format(epoch, (time.time() - t0),\n",
    "                                                       val_loss, math.exp(val_loss)))\n",
    "        print('-' * 100)\n",
    "        print('epoch\\t\\tms/batch\\tlr\\tloss\\tperplexity')\n",
    "\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0.\n",
    "        t0 = time.time()\n",
    "        if epoch == 1: optimizer.param_groups[0]['lr'] = curr_lr = lr # finished warmup\n",
    "        for i, (x,y) in enumerate(train_loader):\n",
    "            if i % log_interval == 0 and i > 0:\n",
    "                cur_loss = total_loss / log_interval\n",
    "                elapsed = time.time() - t0\n",
    "                print('{:3d} ({:2.1f}%)\\t{:5.2f}\\t\\t{:1.3}\\t{:5.2f}\\t{:8.2f}'.format(\n",
    "                    epoch, 100*i/float(len(train_loader)),\n",
    "                    elapsed * 1000 / log_interval, curr_lr, cur_loss, math.exp(cur_loss)))\n",
    "                total_loss = 0\n",
    "                t0 = time.time()\n",
    "\n",
    "            x, y = x.permute(1,0).to(device), y.permute(1,0).to(device)\n",
    "            model.zero_grad()\n",
    "            \n",
    "            yhat = model(x).view(-1, train_data.word_count())\n",
    "            loss = criterion(yhat, y.contiguous().view(-1))\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('Graceful Exit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65f94cd8-db85-4691-a1b4-76d7e7db134e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring best checkpointed model...\n",
      "\n",
      "=========================================================================================\n",
      "| end of training | test loss  4.24 | test perplexity    69.52\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "print('Restoring best checkpointed model...')\n",
    "with open(save, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "\n",
    "test_loss = evaluate(test_data)\n",
    "print('=' * 89)\n",
    "print('| end of training | test loss {:5.2f} | test perplexity {:8.2f}'.format(test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "761e191d-35e8-442e-850d-48629e98b962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uncurated samples\n",
      "-----------------------------------------------------------------------------------------\n",
      "(0) accepted accepted observed accepted accepted accepted accepted accepted accepted\n",
      "accepted accepted accepted accepted accepted accepted accepted accepted accepted\n",
      "accepted accepted accepted accepted accepted accepted rhymes accepted accepted\n",
      "accepted accepted accepted accepted accepted accepted accepted accepted accepted\n",
      "accepted accepted accepted accepted accepted accepted accepted accepted accepted\n",
      "accepted accepted accepted accepted accepted accepted accepted accepted accepted\n",
      "accepted accepted accepted accepted accepted accepted accepted accepted accepted\n",
      "accepted accepted accepted accepted accepted accepted accepted accepted accepted\n",
      "accepted accepted accepted accepted accepted accepted accepted accepted accepted\n",
      "accepted accepted accepted accepted accepted accepted accepted accepted accepted\n",
      "accepted accepted accepted accepted accepted accepted accepted accepted accepted\n",
      "accepted accepted accepted accepted accepted accepted accepted accepted accepted\n",
      "accepted accepted accepted that accepted accepted accepted accepted accepted\n",
      "accepted accepted accepted accepted accepted accepted accepted accepted accepted\n",
      "accepted accepted accepted accepted accepted accepted accepted accepted accepted\n",
      "accepted accepted accepted accepted accepted accepted accepted accepted accepted\n",
      "accepted accepted accepted accepted accepted accepted\n",
      "(1) Randolph Randolph Street Guildford Adelaide went Long Street 90 Guildford built\n",
      ". laid laid laid laid laid laid laid laid laid laid laid laid laid laid laid\n",
      "laid laid laid laid laid laid laid laid laid laid laid laid and laid laid laid\n",
      "laid laid laid laid laid laid laid laid laid laid laid laid laid laid laid laid\n",
      "laid laid laid laid laid laid laid laid laid laid laid laid laid laid laid laid\n",
      "laid laid laid laid laid laid laid laid laid laid laid laid laid laid laid laid\n",
      "laid laid laid laid laid laid laid laid laid laid laid laid laid laid laid laid\n",
      "laid laid laid laid laid laid laid laid laid laid laid laid laid laid laid laid\n",
      "laid laid laid laid laid laid laid laid laid laid laid laid laid laid laid laid\n",
      "laid laid laid laid laid laid laid laid laid laid laid\n",
      "(2) 1778 1778 1778 the the the personally the the that the <unk> the the the in the\n",
      "the the the the the that the the the the the the @-@ the the the the the the the\n",
      "the the \" the \" the the the the the <unk> the the the the ( the the the , the\n",
      "the the the the the the the the the the the the the the the the the the requires\n",
      "the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the the the the the the the the the the the the\n",
      "the the the the the the the the the all the the the\n",
      "(3) 1622 1616 1616 1616 1616 1616 1616 1616 sexual 1616 1616 1616 1616 1616 1616\n",
      "1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616\n",
      "1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616\n",
      "1616 he 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616\n",
      "1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616\n",
      "1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 forced 1616\n",
      "1616 1616 1616 1616 1616 1616 wife 1616 1616 1616 1616 1616 1616 1754 1616 1616\n",
      "1616 1616 1616 1616 1616 1616 1616 1625 1616 1616 1616 1616 1616 1616 1616 1616\n",
      "1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616 1616\n",
      "1616 1616 1616 1616 1616 1616 1616 1616\n",
      "(4) Principe Principe Principe <unk> Principe Principe Principe spend Principe\n",
      "Principe Principe Principe Principe Principe though Principe Principe Principe\n",
      "Principe in Principe Principe Principe Palestro <unk> and had call for <unk>\n",
      "Principe , New Principe and <unk> After Principe Principe and ungulates in\n",
      "Principe Principe Adams and New population even though Principe and vegetation .\n",
      "Principe <unk> even though New New Principe Lewis and New the New New between\n",
      "Fraser and New Principe Arnold <unk> <unk> and for Principe call \" <unk> and had\n",
      "even though Maeda and New Principe . Principe and was communal <unk> . Principe\n",
      "and New Principe , ash . Principe and combined 1959 were combined the Gaboon Lu\n",
      ". <unk> in 42nd rack , on New Principe Buddhist Adams in New New suffered .\n",
      "<unk> geology . Principe and <unk> and counter four <unk> and Principe and\n",
      "reaching <unk> and practice <unk> <unk> and early Laws\n"
     ]
    }
   ],
   "source": [
    "print('\\nUncurated samples')\n",
    "print('-' * 89)\n",
    "\n",
    "def sample():\n",
    "    words = []\n",
    "    model.eval()\n",
    "    history = torch.randint(train_data.word_count(), (1, 1), dtype=torch.long).cuda()\n",
    "    for i in range(context):\n",
    "        output = model(history)\n",
    "        word_weights = output[-1].squeeze().exp().cpu()\n",
    "        word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "        word_tensor = torch.Tensor([[word_idx]]).long().cuda()\n",
    "        history = torch.cat([history, word_tensor], 0)\n",
    "\n",
    "        words.append(train_data.idx2word[word_idx])\n",
    "\n",
    "    return '\\n'.join(textwrap.wrap(' '.join(words),80))\n",
    "\n",
    "for i in range(5):\n",
    "    print('({})'.format(i), sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66a4f538-8c9e-48ff-aa5b-031557cb53aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e66541",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
