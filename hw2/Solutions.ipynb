{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76f6f14f-ada4-4e34-8808-6ff1534445b4",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "Lucas Bezerra, ID: 171412,\n",
    "lucas.camaradantasbezerra@kaust.edu.sa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7c33b8-1ee2-40a9-a912-085694a83a70",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Problem 1: Transformer Questions\n",
    "\n",
    "<strong>Q1 (10 points): In the above self-attention operation, why we need to incorporate the scale factor âˆšdk into calculation?</strong>\n",
    "\n",
    "The scale factor is used for normalizing the dot product $Q\\,K^T$, making sure it has the same variance as the individual components of $Q$ and $K$.\n",
    "\n",
    "Assume these components are random variables $q_i, k_i\\;,\\forall i\\in\\{1,2,\\dots,d_k\\}$ with mean 0 and variance $\\sigma^2$. Then, it stems from the Central Limit Theorem that the dot product between $\\mathbf{q}$ and $\\mathbf{k}$ is normally-distributed:\n",
    "\n",
    "$$ \\mathbf{q}\\cdot \\mathbf{k} = \\sum\\limits_{i=1}^{d_k} q_i\\cdot k_i \\sim \\mathcal{N}\\left( 0, d_k\\cdot \\sigma^2 \\right) $$\n",
    "\n",
    "$$ \\frac{1}{\\sqrt{d_k}} \\mathbf{q}\\cdot \\mathbf{k} \\sim \\mathcal{N}\\left( 0,\\sigma^2 \\right) $$\n",
    "\n",
    "This ensures the network will have similar ranges of parameters no matter what dimensionality is chosen, and avoids vanishing gradient problems (keeping the variance of $\\mathbf{q}\\cdot \\mathbf{k}$ low ensures that $\\text{softmax}(\\mathbf{q}\\cdot \\mathbf{k})$ is also close to the origin, and thus in a high-gradient region)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289e0a64-5051-4e8a-9d5e-021cfc19bd7a",
   "metadata": {},
   "source": [
    "<strong>Q2 (10 points): When we train the Transformer on the word sequences, usually we need to add additional positional embedding for each word, why is this necessary?</strong>\n",
    "\n",
    "Without position encoding, the attention is an ordering-agnostic function, meaning that the ordering of words in a sentence does not change the outcome. However, in any language the order of words matters. That is why a positional embedding is added to each word embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73ea51a-9f42-4be0-a596-5ee919243a5f",
   "metadata": {},
   "source": [
    "<strong>Q3 (10 points): In the Transformer framework, there are two types of attention modules, which are self-attention and encoder-decoder attention. What is the difference between these two modules in terms of functionality and technical implementation?</strong>\n",
    "\n",
    "TO DO ---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6362d6b-bb27-4741-bbf3-4549ec5d1bfa",
   "metadata": {},
   "source": [
    "<strong>Q4 (10 points): There are also other types of attention calculations such as the additive attention [4]. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. In the Transformer model, why the authors choose to use scaled-dot product attention instead of additive attention and what is the main advantages?</strong>\n",
    "\n",
    "In [2] the authors argue that, although both attention mechanisms are similar in theoretical complexity, the multiplicative attention is computationally more efficient, since it consists simple matrix multiplications, an operation that runs very fast in specialized hardware (e.g. GPUs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e731b9e-e496-4ee7-beb9-5e0dba258c20",
   "metadata": {},
   "source": [
    "<strong>Q5 (10 points): BERT and GPT model pretrain their model on a largescale dataset in a self-supervising way. Please describe their pretraining tasks and discuss why it is useful.</strong>\n",
    "\n",
    "TO DO ---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b49473-e200-4cac-b27d-50f0331c8dca",
   "metadata": {},
   "source": [
    "<strong>Q6 (10 points) : In the BERT model design, there are two special tokens \\[CLS\\] and [SEP], what is the purpose of designing these two special tokens and how they are used during the training and evaluation.</strong>\n",
    "\n",
    "TO DO ---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b505b0b7-3345-42da-aa76-9450e6964549",
   "metadata": {},
   "source": [
    "## Problem 2: Coding Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1558601d-3ca5-410b-8389-69a3761c3869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,time,math,textwrap\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import dataset, transformer\n",
    "\n",
    "root = 'data/wikitext-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3443b8-708a-416a-a437-46dbd1968c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .00035\n",
    "context = 150\n",
    "batch_size = 32\n",
    "log_interval = 50\n",
    "\n",
    "heads = 10\n",
    "depth = 16\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795a5ec3-6795-40c1-a156-c5758225ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset.WikiText2(root, context, dataset.DatasetSplit.train)\n",
    "valid_data = dataset.WikiText2(root, context, dataset.DatasetSplit.valid)\n",
    "test_data = dataset.WikiText2(root, context, dataset.DatasetSplit.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9648ee1-44b8-4fd5-8c0e-bb6461db33ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0.\n",
    "        loader = torch.utils.data.DataLoader(dataset=data,batch_size=batch_size,shuffle=False)\n",
    "        for i, (x,y) in enumerate(loader):\n",
    "            x, y = x.permute(1,0).to(device), y.permute(1,0).to(device)\n",
    "            yhat = model(x).view(-1, train_data.word_count())\n",
    "            loss += criterion(yhat, y.contiguous().view(-1))\n",
    "\n",
    "    print()\n",
    "    model.train()\n",
    "    return loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f25b3d-a2bc-4fab-97e1-d08048c61f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformer.Transformer(context, train_data.word_count(), 400, 40, 900, heads, depth, tied_weights=True).to(device)\n",
    "count = sum([np.prod(parm.shape) for parm in model.parameters() if parm.requires_grad])\n",
    "print('Initialized graph with {} parameters'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6e01f7-00a7-42a4-882c-2c4cab395bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "curr_lr = .0001\n",
    "clip = .25\n",
    "best_val_loss = None\n",
    "epochs = 10\n",
    "save = 'model.pt'\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data,batch_size=batch_size,shuffle=True)\n",
    "print('Initiating training, {} iterations/epoch.'.format(len(train_loader)))\n",
    "\n",
    "try:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=curr_lr)\n",
    "    for epoch in range(epochs):\n",
    "        t0 = time.time()\n",
    "        val_loss = evaluate(valid_data)\n",
    "        print('-' * 100)\n",
    "        print('| checkpoint | epoch {:3d} | time: {:5.2f}s | validation loss {:5.2f} | '\n",
    "                'validation perplexity {:8.2f}'.format(epoch, (time.time() - t0),\n",
    "                                                       val_loss, math.exp(val_loss)))\n",
    "        print('-' * 100)\n",
    "        print('epoch\\t\\tms/batch\\tlr\\tloss\\tperplexity')\n",
    "\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0.\n",
    "        t0 = time.time()\n",
    "        if epoch == 1: optimizer.param_groups[0]['lr'] = curr_lr = lr # finished warmup\n",
    "        for i, (x,y) in enumerate(train_loader):\n",
    "            if i % log_interval == 0 and i > 0:\n",
    "                cur_loss = total_loss / log_interval\n",
    "                elapsed = time.time() - t0\n",
    "                print('{:3d} ({:2.1f}%)\\t{:5.2f}\\t\\t{:1.3}\\t{:5.2f}\\t{:8.2f}'.format(\n",
    "                    epoch, 100*i/float(len(train_loader)),\n",
    "                    elapsed * 1000 / log_interval, curr_lr, cur_loss, math.exp(cur_loss)))\n",
    "                total_loss = 0\n",
    "                t0 = time.time()\n",
    "\n",
    "            x, y = x.permute(1,0).to(device), y.permute(1,0).to(device)\n",
    "            model.zero_grad()\n",
    "            yhat = model(x).view(-1, train_data.word_count())\n",
    "            loss = criterion(yhat, y.contiguous().view(-1))\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('Graceful Exit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f94cd8-db85-4691-a1b4-76d7e7db134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Restoring best checkpointed model...')\n",
    "with open(save, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "\n",
    "test_loss = evaluate(test_data)\n",
    "print('=' * 89)\n",
    "print('| end of training | test loss {:5.2f} | test perplexity {:8.2f}'.format(test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761e191d-35e8-442e-850d-48629e98b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nUncurated samples')\n",
    "print('-' * 89)\n",
    "\n",
    "def sample():\n",
    "    words = []\n",
    "    model.eval()\n",
    "    history = torch.randint(train_data.word_count(), (1, 1), dtype=torch.long).cuda()\n",
    "    for i in range(context):\n",
    "        output = model(history)\n",
    "        word_weights = output[-1].squeeze().exp().cpu()\n",
    "        word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "        word_tensor = torch.Tensor([[word_idx]]).long().cuda()\n",
    "        history = torch.cat([history, word_tensor], 0)\n",
    "\n",
    "        words.append(train_data.idx2word[word_idx])\n",
    "\n",
    "    return '\\n'.join(textwrap.wrap(' '.join(words),80))\n",
    "\n",
    "for i in range(5):\n",
    "    print('({})'.format(i), sample())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
