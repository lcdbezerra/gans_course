{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe5f86b-d03d-4a78-940a-3a9a987349ac",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "\n",
    "Lucas Bezerra, ID: 171412, lucas.camaradantasbezerra@kaust.edu.sa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f67b23-587d-4d87-b597-358124dee2ae",
   "metadata": {},
   "source": [
    "### Problem 1: Recurrent Neural Network\n",
    "\n",
    "<strong>a. (10 points) LSTM contains a forget gate to decide how many previous\n",
    "information need to be kept. There is a Sigmoid activation in the forget gate.\n",
    "Explain why we use Sigmoid here, instead of Tanh.</strong>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4edcfe-9748-4e6f-ac81-50bf3cb2f166",
   "metadata": {},
   "source": [
    "<strong>b. (10 points) In neural language processing and many other fields that\n",
    "process sequential data, more and more LSTM-based models are replaced by\n",
    "Transformer-based models [2] nowadays. Why LSTM is not as popular as before\n",
    "now? List at least 2 drawbacks of LSTM-compared to Transformer. Explain\n",
    "each of them with 1 to 3 sentences.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164785f0-90ed-46e1-a7ab-dc5a0531d637",
   "metadata": {},
   "source": [
    "### Problem 2: DCGAN\n",
    "\n",
    "<strong>In this coding assignment, you need to implement the discriminator of the DCGAN (and maybe tune some hyperparameters\n",
    " like the learning rates and the number of epochs)\n",
    "to make your model generates some MNIST numbers. You can find a basic draft of the discriminator in the model.py\n",
    "file. Fill the draft and train the model.</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dfe8ee-a582-4fd1-9b31-5e05e0f3a281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data_utils\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "\n",
    "from model import Generator, Discriminator\n",
    "from train import train_DCGAN\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356bd9db-cb5f-4347-8981-62ca92beab9c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Hyperparameters are listed here. You maybe need to play a little bit with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325bc5db-eb4f-45f2-b253-0d28e9894b4a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "g_lr = 0.002\n",
    "d_lr = 0.0002\n",
    "batch_size = 128\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101dd5bc-3e71-4779-9e8a-d83c241a1891",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trans = transforms.Compose([\n",
    "            transforms.Scale(32),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, ), (0.5, )),\n",
    "        ])\n",
    "\n",
    "train_set = MNIST(root='.', train=True, transform=trans, download=True)\n",
    "train_loader = data_utils.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77313ff2-fba3-465a-83ee-c6a135217fed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The training part. Due to the easy dataset and simplified model structure, the training is relatively fast.\n",
    "# In my case (Single Titan RTX) the training is finished in about 10 minutes.\n",
    "# You may need to balance the training of generator and discriminator (e.g. by tuning their learning rates) to\n",
    "# avoid a discriminator that is much stronger than the generator during training\n",
    "\n",
    "\n",
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "optim_G = torch.optim.Adam(G.parameters(), lr=g_lr, betas=(0.5, 0.999))\n",
    "optim_D = torch.optim.Adam(D.parameters(), lr=d_lr, betas=(0.5, 0.999))\n",
    "\n",
    "loss_f = nn.BCELoss()\n",
    "\n",
    "train_DCGAN(G, D, optim_G, optim_D, loss_f, train_loader, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280df656-377d-43d3-944b-8b0adc4e329b",
   "metadata": {},
   "source": [
    "### Problem 3: Mode Collapse\n",
    "\n",
    "<strong>a. (5 points) Explain the mode collapse problem happens in GANs in 2-3\n",
    "sentences.</strong></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d14df9d-ec0b-40e6-83da-ae9e4aa0bb46",
   "metadata": {},
   "source": [
    "<strong>b. (10 points) UnRolled GAN [4] alleviates the mode collapse problem of\n",
    "GAN training by forecasting the future K steps of which networks? Explain\n",
    "your choice in 1-2 sentences. A. Generator B. Discriminator</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cf3519-7b69-4768-b7ad-3a45d7736394",
   "metadata": {},
   "source": [
    "<strong>c. (10 points) MAD-GAN [5] uses several generators to alleviate the mode collapse problem. Given a fake data, the discriminator needs to recognize the generator that produces it. Why this helps to alleviate the mode collapse problem?</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b5c901-aac2-4c42-b340-25959f01e1e8",
   "metadata": {},
   "source": [
    "<strong>d. (15 points) Wasserstein GAN [6, 7] is designed to train the generator by minimizing the Wasserstein distance (AKA earth moverâ€™s distance) between the real data distribution and generated data distribution. WGAN helps stable the training of GAN and alleviate the mode collapse issue. Explain why Wasserstein distance is better than KL/JS divergence when there is no overlap between 2 distributions? How was this notion used to develop the Wasserstein GAN Discriminator D and Generator G losses compared to standard GAN.</strong>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
