{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a61a5cab-358c-4083-b737-5946403f1835",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "Lucas Bezerra, 171412, lucas.camaradantasbezerra@kaust.edu.sa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36f8842-75c7-4d01-ae74-ca9303300885",
   "metadata": {},
   "source": [
    "### 1. Zero-Shot Learning and Vision-Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f839bb-654a-483d-9841-828123c2d689",
   "metadata": {},
   "source": [
    "1.1 Basic Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8855d057-4ef1-4c6c-a354-4c091387e7e7",
   "metadata": {},
   "source": [
    "- <strong>What are the differences between GAZSL [2] and CIZSL [1]?</strong>\n",
    "\n",
    "In CIZSL an extra augmentation method is added. Besides the regular adversarial loss, the creativity-inspired loss is added when training the generator: hallucinated class descriptions $t^h$ are generated as linear combinations of the training set class descriptions ($t^h = \\alpha t_a^s + (1-\\alpha)t_b^s\\;,\\;\\alpha \\in \\left[0.2,0.8\\right]$). Then, for any $t^h \\sim p_{text}^h, z \\sim p_z$:\n",
    "\n",
    "1. Maximize the likelihood that the generator output (given hallucinated text $t^h, z$) is classified as real by the discriminator\n",
    "2. Maximize the entropy of the discriminator classifier the generator output (given hallucinated text $t^h, z$). This encourages the generator to create images that can't be classified by the discrminator as belonging to any particular class, thus preventing the generator from creating images that belong to any particular class.\n",
    "\n",
    "This extra augmentation heavily improves the performance of the model as compared to GAZSL.\n",
    "\n",
    "<span style=\"color:red\">Anything else?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d95d9d0-c68e-4e13-9c18-84fc0526eb82",
   "metadata": {},
   "source": [
    "- <strong>How the creativity loss is connected with the classification head over\n",
    "classes? Why it can be helpful?</strong>\n",
    "\n",
    "As stated in the previous question, the creativity loss also encourages the generator to create images that the discriminator can't properly classify. This is helpful because it prevents the generator to creating images that belong to any of the classes it saw in the dataset, and thus encouraging creativity when generating new images, but not too much creativity, since the discriminator still needs to be tricked into believing the images are real, as in the traditional GAN setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c841921-210f-4bf2-8598-0e8cfa4495f9",
   "metadata": {},
   "source": [
    "- Run the code of CIZSL on one text-based dataset (e.g., CUB-wiki).\n",
    "Please report your performance using the provided hyperparameters (your\n",
    "performance may slightly different from the reported due to instability and\n",
    "different hyper-parameters). You can find the code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a472bb-4510-4fb4-ae7d-a94020646481",
   "metadata": {},
   "source": [
    "Interpolation between two real text features:\n",
    "\n",
    "Implement SM Divergence suitable for our case: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a76b92f0-44d1-458a-a47e-b2ac0ac178f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/camaral/code/gans_course/hw5/CIZSL\n",
      "Namespace(dataset='CUB', splitmode='hard', model_number=2, exp_name='Reproduce', main_dir='./', creativity_weight=0.1, validate=0, SM_Alpha=0.5, SM_Beta=0.9999, gpu='0', manualSeed=None, resume=None, disp_interval=20, save_interval=200, evl_interval=10)\n",
      "Random Seed:  2010\n",
      "100%|█████████████████████████████████████| 3001/3001 [2:15:51<00:00,  2.72s/it]\n",
      "===============\n",
      "===============\n",
      "Reproduce CUB hard\n",
      "Accuracy is 14.26%, and Generalized AUC is 11.53%\n",
      "Namespace(dataset='NAB', splitmode='hard', model_number=2, exp_name='Reproduce', main_dir='./', creativity_weight=0.1, validate=0, SM_Alpha=0.5, SM_Beta=0.9999, gpu='0', manualSeed=None, resume=None, disp_interval=20, save_interval=200, evl_interval=10)\n",
      "Random Seed:  6154\n",
      "100%|█████████████████████████████████████| 3001/3001 [8:51:46<00:00, 10.63s/it]\n",
      "===============\n",
      "===============\n",
      "Reproduce NAB hard\n",
      "Accuracy is 8.936%, and Generalized AUC is 6.715%\n",
      "Namespace(dataset='NAB', splitmode='easy', model_number=2, exp_name='Reproduce', main_dir='./', creativity_weight=1.0, validate=0, SM_Alpha=0.5, SM_Beta=0.9999, gpu='0', manualSeed=None, resume=None, disp_interval=20, save_interval=200, evl_interval=10)\n",
      "Random Seed:  6436\n",
      "100%|█████████████████████████████████████| 3001/3001 [8:58:02<00:00, 10.76s/it]\n",
      "===============\n",
      "===============\n",
      "Reproduce NAB easy\n",
      "Accuracy is 34.41%, and Generalized AUC is 22.02%\n",
      "Namespace(dataset='CUB', splitmode='easy', model_number=2, exp_name='Reproduce', main_dir='./', creativity_weight=0.0001, validate=0, SM_Alpha=0.5, SM_Beta=0.9999, gpu='0', manualSeed=None, resume=None, disp_interval=20, save_interval=200, evl_interval=10)\n",
      "Random Seed:  2380\n",
      "100%|█████████████████████████████████████| 3001/3001 [2:20:01<00:00,  2.80s/it]\n",
      "===============\n",
      "===============\n",
      "Reproduce CUB easy\n",
      "Accuracy is 40.98%, and Generalized AUC is 38.65%\n"
     ]
    }
   ],
   "source": [
    "# %%bash\n",
    "%cd CIZSL\n",
    "!./run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daef28b-99c4-4c13-9950-d284b8ba927c",
   "metadata": {},
   "source": [
    "### 2. Score-Based Generative Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478015f1-65ed-4b34-8d64-7b9110c4698d",
   "metadata": {},
   "source": [
    "2.1 Basic Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0553188e-5e07-432a-a012-7f6dd2802ad1",
   "metadata": {},
   "source": [
    "- <strong>Describe the pipeline logic of [4] (i.e., forward and backward steps).</strong>\n",
    "\n",
    "The forward process consists of a diffusion process, a Stochastic Differential Equation (SDE), that adds noise to the data distribution ($p_0(x)$) slowly, a bit in each layer, up to the last layer ($p_T(x)$) where the distribution is just noise and no more of the original data is left.\n",
    "\n",
    "The diffusion process can be reversed by applying another diffusion process that can be obtained by estimating the score: $\\nabla_x log\\,p_t(x), t \\in \\left[ 0,T\\right]$. The score estimate $s_\\theta(x(t),t)$ is learned during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f27f9-03b9-4057-806c-d90c228fd684",
   "metadata": {},
   "source": [
    "- <strong>What is Energy-Based Models (EBMs) and Score-Based Generative Models (SBGMs)?</strong>\n",
    "\n",
    "Energy-based models are those that model a data distribution in the form:\n",
    "\n",
    "$$ p_\\theta(x) = \\frac{e^{-E_\\theta(x)}}{Z_\\theta},\\;\\text{where: } Z_\\theta = \\int e^{-E_\\theta(x)} dx$$\n",
    "\n",
    "$Z_\\theta$ is not tractable as it involves computing an integral over all dimensions of the data, thus the usual way to go is to compute $\\nabla_\\theta log\\,p_\\theta(x) = - \\nabla_\\theta E_\\theta(x) + \\mathbb{E}_{x\\sim p_\\theta(x)}\\left[\\nabla_\\theta E_\\theta(x) \\right]$ and use gradient ascent to maximize the log-likelihood of $E_\\theta(x)$ given a dataset.\n",
    "\n",
    "However, even if a model for the distribution is given, it still is hard to sample from it. Score-based sampling techniques such as Langevin MCMC estimate the score $\\nabla_x log\\,p_t(x)$ and use it to sample from a data distribution without the need to estimate the data distribution itself. Methods that use score-based sampling for generating data that is similar to prior dataset are called Score-Based Generative Models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34807e97-d690-44ef-a66c-3b08b9a9d2d1",
   "metadata": {},
   "source": [
    "- <strong>What is the difference among Euler-Maruyama sampling, Langevin MCMC sampling, and Predictor-Corrector (PC) sampling?</strong>\n",
    "\n",
    "The Euler-Maruyama sampling method applies the Euler-Maruyama SDE solver to come up with a solution for the reverse diffusion model (that takes a sample from the prior to the data distribution), which means generating a new sample given a sample from the prior.\n",
    "\n",
    "The Langevin MCMC method generates samples by iterating over the equation:\n",
    "\n",
    "$$x_i^m = x_i^{m-1} + \\varepsilon_i s_\\theta (x_i^{m-1}, \\sigma_i) + \\sqrt{2 \\varepsilon_i} z_i^m,\\quad m=1,2,3,\\dots,M$$\n",
    "\n",
    "It starts from $x_i$ and iterates over it until reaching at the final generated sample: $x_i^M$.\n",
    "\n",
    "The Predictor-Corrector (PC) class of samplers generalize over the past 2 methods: the Predictor can be any SDE solver (e.g. Euler-Maruyama) and the Corrector can be any score-based MCMC approach (e.g. Langevin MCMC). The proposed PC samplers are tailored for reverse diffusion sampling, where they perform better than the previously mentioned techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c597b8-6d5d-403d-9ff8-c9016422b95a",
   "metadata": {},
   "source": [
    "- <strong>What is the difference among VE SDE, VP SDE, and sub-VP SDE?</strong>\n",
    "\n",
    "Variance Exploding (VE) SDE: The continuous-time version of the perturbation kernels used in SMLD. It is called variance exploding because it shows such property as $t\\to \\infty$. It is given by:\n",
    "\n",
    "$$ dx = \\sqrt{\\frac{d\\left[\\sigma^2(t)\\right]}{dt}} dw $$\n",
    "\n",
    "Variance Preserving (VP) SDE: The continuous-time version of the perturbation kernels used in DDPM. It is called variance preserving as its variance is always one provided the initial distribution also has unit variance. It is given by:\n",
    "\n",
    "$$ dx = -\\frac{1}{2}\\beta(t)x\\,dt + \\sqrt{\\beta(t)}dw $$\n",
    "\n",
    "Sub-VP SDE: A new type of SDEs proposed by the authors, that has the property of having its variance always bounded by the VP-SDE at every intermediate step. It is given by:\n",
    "\n",
    "$$ dx = -\\frac{1}{2}\\beta(t)x\\,dt + \\sqrt{\\beta(t)\\left(1-e^{-2\\int_0^t \\beta(s)ds}\\right)}dw $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466589aa-df01-4878-b4ec-61e4f449e225",
   "metadata": {},
   "source": [
    "- <strong>How and why SDE is connected with SBGMs?</strong>\n",
    "\n",
    "Since the transformation from data distribution to prior distribution can be modelled as a diffusion model, the reverse is also a diffusion model. As diffusion models are SDEs, they can be solved using regular SDE solvers (e.g. Euler-Maruyama), including score-based ones (e.g. Langevin MCMC)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15c5277-6bd9-421f-936e-83d105f73c74",
   "metadata": {},
   "source": [
    "- <strong>How the likelihood is computed in probabilistic flow ODE? Why this can not be done for normal SDE?</strong>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010246d3-4bed-4b19-956b-db919b89bb0c",
   "metadata": {},
   "source": [
    "- <strong>Clarify potential disadvantages of discrete noisy perturbation (Hint,\n",
    "suggest reading [5].)</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d99d4a6-2165-4efc-9b7c-6e9db9e53e47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
